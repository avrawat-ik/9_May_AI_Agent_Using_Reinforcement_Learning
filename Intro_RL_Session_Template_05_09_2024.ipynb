{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "34ac95bc-13d6-4df7-b638-b32a885a8fba",
      "metadata": {
        "id": "34ac95bc-13d6-4df7-b638-b32a885a8fba"
      },
      "source": [
        "#  Build Your First AI Agent Using Reinforcement Learning\n",
        "\n",
        "In this session we will provide a hands-on introduction to reinforcement learning. The specific agenda is as follows:\n",
        "- First we will demonstrate the use of the Gymnasium (formerly OpenAI Gym) platform as a testbed for reinforcement learning.\n",
        "- We will then implement and provide intuition behind the Q-learning algorithm, which is a popular technique in reinforcement learning.\n",
        "- Finally we will demonstrate our implementation of Q-learning on two separate environments within Gymnasium, where the algorithm will learn to skillfully perform both tasks.\n",
        "\n",
        "What we will not cover here:\n",
        "- Details and theoretical justification behind the Q-learning algorithm.\n",
        "- Other reinforcement learning algorithms aside from Q-learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b38fb31e-3002-4add-a92f-b35e1a3220f8",
      "metadata": {
        "id": "b38fb31e-3002-4add-a92f-b35e1a3220f8"
      },
      "outputs": [],
      "source": [
        "# <-- INSTALL GYMNASIUM HERE -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aabe4efe-ef3b-4cfb-8d94-4b401a893984",
      "metadata": {
        "id": "aabe4efe-ef3b-4cfb-8d94-4b401a893984"
      },
      "outputs": [],
      "source": [
        "# <-- IMPORT GYMNASIUM HERE -->\n",
        "\n",
        "import time\n",
        "import random\n",
        "from IPython import display\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8f2b8c8-e457-4034-9bd3-89f369b58322",
      "metadata": {
        "id": "a8f2b8c8-e457-4034-9bd3-89f369b58322"
      },
      "source": [
        "---\n",
        "\n",
        "In this session we will look at the [Taxi environment](https://gymnasium.farama.org/environments/toy_text/taxi/) in Gymnasium. In this environment, the goal is to navigate the taxi to the passenger, pick up the passenger, navigate to the passenger's destination, then drop off the passenger. This is achieved by performing the correct sequence of actions from among the possible actions \"move up\", \"move down\", \"move left\", \"move right\", \"pick up passenger\", and \"drop off passenger\".\n",
        "\n",
        "In reinforcement learning, our goal is to learn a *policy* for successfully performing a task. A policy decides what *action* to take at each step based on an observation of the system's overall *state*. Each time we perform an action we receive a *reward* that is aligned with the overall objective. We will learn a policy by interacting with the system to determine which actions, when performed in each state, lead us to the higest rewards.\n",
        "\n",
        "To get a clearer understanding of states, actions and rewards, let's look at the states we observe and rewards we receive when we take random actions in the Taxi environment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4b2c421-f2d5-4aac-8293-74c3e2ce278d",
      "metadata": {
        "id": "a4b2c421-f2d5-4aac-8293-74c3e2ce278d"
      },
      "source": [
        "**NOTE:** Animations are best viewed by running this notebook locally (outside of Colab)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8ca4b64-14e9-42bb-afc3-e323e3fc8ef4",
      "metadata": {
        "id": "c8ca4b64-14e9-42bb-afc3-e323e3fc8ef4"
      },
      "outputs": [],
      "source": [
        "# Generate some descriptive names to display for the acions\n",
        "action_desc = {\n",
        "    0: \"Move south (down)\",\n",
        "    1: \"Move north (up)\",\n",
        "    2: \"Move east (right)\",\n",
        "    3: \"Move west (left)\",\n",
        "    4: \"Pickup passenger\",\n",
        "    5: \"Drop off passenger\"\n",
        "}\n",
        "\n",
        "# Create the Taxi environment\n",
        "# <-- CREATE A GYMNASIUM Taxi-v3 ENVIRONMENT HERE -->\n",
        "\n",
        "# Initilize the environment and draw the current state\n",
        "# <-- READ THE INITAL STATE FROM THE ENVIRONMENT HERE -->\n",
        "\n",
        "# Render the environment\n",
        "plt.imshow(env.render())\n",
        "plt.show()\n",
        "\n",
        "# Loop for 100 time steps\n",
        "for i in range(100):\n",
        "\n",
        "    # Select a random action\n",
        "    # <-- SAMPLE A RANDOM ACTION FROM THE ACTION SPACE HERE -->\n",
        "\n",
        "    # Apply the action, then observe the state and reward\n",
        "    # <-- APPLY THE ACTION HERE -->\n",
        "\n",
        "    # Draw the new state\n",
        "    display.clear_output(wait=True)\n",
        "    plt.imshow(env.render())\n",
        "\n",
        "    # Add a caption indicating the curent state, action, and reward\n",
        "    rect = matplotlib.patches.Rectangle((150,0), 250, 75, facecolor=\"#999999\", edgecolor=\"#000000\")\n",
        "    ax = plt.gca()\n",
        "    ax.add_patch(rect)\n",
        "    plt.text(165,25,f\"State: {obs}\")\n",
        "    plt.text(165,45,f\"Action: {action_desc[action]}\")\n",
        "    plt.text(165,65,f\"Reward: {reward}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Close the environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718d663e-a973-495c-9bec-fd8c35175b5b",
      "metadata": {
        "id": "718d663e-a973-495c-9bec-fd8c35175b5b"
      },
      "source": [
        "How can we learn the correct sequence of actions to perform just by testing out different actions and observing rewards?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "We will introduce the concept of *Q-learning* by looking at a simplified version of the algorithm on a much simpler version of the taxi problem."
      ],
      "metadata": {
        "id": "5YpjWbzNLg4z"
      },
      "id": "5YpjWbzNLg4z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c06e01-07bd-4acb-a3f8-4d0d2a2cf823",
      "metadata": {
        "id": "44c06e01-07bd-4acb-a3f8-4d0d2a2cf823"
      },
      "outputs": [],
      "source": [
        "def plot_grid_state(s, n, Q):\n",
        "    \"\"\"\n",
        "    This function will be used to visualize the current state and\n",
        "    Q table for the simplified taxi example.\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_size_inches(12, 2)\n",
        "\n",
        "    for i in range(n):\n",
        "        if i == n-1:\n",
        "            facecolor = \"#00ff00\"\n",
        "        else:\n",
        "            facecolor = \"#ffffff\"\n",
        "        rect = matplotlib.patches.Rectangle((20*i,0), 20, 20, facecolor=facecolor, edgecolor=\"#000000\")\n",
        "        ax.add_patch(rect)\n",
        "        if (i,0) in Q:\n",
        "          plt.text(20*i+5,30,\"<-- {}\".format(Q[(i,0)]))\n",
        "          plt.text(20*i+5,24,\"{} -->\".format(Q[(i,1)]))\n",
        "\n",
        "    plt.xlim([-5, 20*n+5])\n",
        "    plt.ylim([-5, 30])\n",
        "    ax.set_aspect('equal', adjustable='box')\n",
        "    ax.axis('off')\n",
        "\n",
        "    taxi1 = matplotlib.patches.Rectangle((20*s+3,5), 14, 7, facecolor=\"#ffff00\", edgecolor=\"#000000\")\n",
        "    taxi2 = matplotlib.patches.Circle((20*s+6,5), 2, facecolor=\"#000000\", edgecolor=\"#000000\")\n",
        "    taxi3 = matplotlib.patches.Circle((20*s+14,5), 2, facecolor=\"#000000\", edgecolor=\"#000000\")\n",
        "    ax.add_patch(taxi1)\n",
        "    ax.add_patch(taxi2)\n",
        "    ax.add_patch(taxi3)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_grid_state(2,15,{})"
      ],
      "metadata": {
        "id": "nFPz3QYSE4kE"
      },
      "id": "nFPz3QYSE4kE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this simplified problem, the goal is to navigate the taxi on a 1-dimensional grid to the green square on the right end of the grid. We can perform two actions , \"move left\" and \"move right\". We incur a cost of 1 unit (or a reward of -1 unit) for every step we take prior to reaching the green square, and zero cost once we reach (and stay at) the green square.\n",
        "\n",
        "At each time step we are told the ID of the grid cell that we are in, we select one of the two actions, and we are told the cost that we incurred (or reward we earned)."
      ],
      "metadata": {
        "id": "WLNiP9ptE1Q-"
      },
      "id": "WLNiP9ptE1Q-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "We will use the algorithm described below. In this algorithm we will maintain a table that contains an entry for each grid cell and action. Each entry will contain a grid cell ID, an action, and a numerical value. For example, several rows of this table could look as follows:\n",
        "\n",
        "| Grid cell ID | Action | Value |\n",
        "| :--- | :--- | :--- |\n",
        "| 3 | Left | 7 |\n",
        "| 3 | Right | 5 |\n",
        "| 4 | Left | 6 |\n",
        "| 4 | Right | 6 |\n",
        "\n",
        "The steps of this algorithm are the following:\n",
        "\n",
        "1. Initially start with an empty table.\n",
        "2. Get the ID of the initial grid cell.\n",
        "3. Add entries to the table for the initial grid cell ID and both actions. Initialize the numerical value to zero for these actions.\n",
        "2. Repeat:\n",
        "  - Get the ID of the current grid cell.\n",
        "  - Select the action with the smallest value in the table for the current grid cell ID. If both values are equal, select an action at random.\n",
        "  - Perform the selected action, observe the cost, and observe the ID of the next grid cell that we will visit.\n",
        "  - If there is no entry in the table for the next grid cell ID, add entries to the table for this cell ID and both actions. Initialize the numerical value to zero for these actions.\n",
        "  - Update the numerical value in the table for the current grid cell ID and action that we selected. **The new value will be the observed cost plus the smaller of the two values in the table for the next grid cell ID.**\n",
        "  - Update the current grid cell ID with the next grid cell ID."
      ],
      "metadata": {
        "id": "JGAKZFAHGfOa"
      },
      "id": "JGAKZFAHGfOa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4c8ca1c-47df-4b2f-90ec-2917a678850a",
      "metadata": {
        "id": "b4c8ca1c-47df-4b2f-90ec-2917a678850a"
      },
      "outputs": [],
      "source": [
        "# Choose a grid containing 15 cells\n",
        "n = 15\n",
        "\n",
        "# Initialize the state to the leftmost cell\n",
        "s = 0\n",
        "\n",
        "# Create an empty Q table\n",
        "# <-- CREATE AN EMPTY Q TABLE HERE -->\n",
        "\n",
        "# Initialize the Q values for the initial state\n",
        "# <-- INITIALIZE Q-VALUES FOR THE INITIAL STATE HERE -->\n",
        "\n",
        "# Loop over 15 episodes\n",
        "# Each episode ends when the taxi reaches the green cell\n",
        "episodes = 0\n",
        "while episodes < 15:\n",
        "\n",
        "  time.sleep(0.1)\n",
        "\n",
        "  # Display the current state\n",
        "  display.clear_output(wait=True)\n",
        "  plot_grid_state(s,15,Q)\n",
        "\n",
        "  #input()\n",
        "\n",
        "  # If we reached the goal, re-initialize the state\n",
        "  if s == n-1:\n",
        "    time.sleep(1)\n",
        "    episodes += 1\n",
        "    s = 0\n",
        "\n",
        "  # Select an action\n",
        "  # <-- SELECT AN ACTION HERE -->\n",
        "\n",
        "  # Update our position\n",
        "  if a == 0:\n",
        "    s_next = max(0,s-1)\n",
        "  else:\n",
        "    s_next = min(s+1,n-1)\n",
        "\n",
        "  # Add Q_next to the table if not yet in the table\n",
        "  # <-- ADD NEW Q VALUES HERE -->\n",
        "\n",
        "  # Update the Q table\n",
        "  # <-- UPDATE EXISTING VALUES HERE -->\n",
        "\n",
        "  # Set the current state to be the next observed state\n",
        "  s = s_next"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "049d41a2-6c4a-4bde-9219-e0a4b4fb80a8",
      "metadata": {
        "id": "049d41a2-6c4a-4bde-9219-e0a4b4fb80a8"
      },
      "source": [
        "---\n",
        "\n",
        "The *Q-learning* algorithm is conceptually very similar to the simple algorithm that we introduced above. We provide a general implementation of the Q-learning algirithm below. There are some small differences that we wont discuss in detail here (but cover in detail in the IK modules on reinforcement learning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f0ae646-846d-4d17-94bb-943859228c09",
      "metadata": {
        "id": "3f0ae646-846d-4d17-94bb-943859228c09"
      },
      "outputs": [],
      "source": [
        "class QLearner:\n",
        "    \"\"\"\n",
        "    This class will allow us to specify a Gymnasium environment and apply\n",
        "    tabular Q-learning on it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, environment, g=0.98, a=0.05, e=0.05):\n",
        "        # Initialize the following:\n",
        "        # g: The discount factor used in our total discounted reward\n",
        "        # a: The learning rate for Q-learning\n",
        "        # e: The epsilon for epsilon-greedy action selection\n",
        "        self.g = g\n",
        "        self.a = a\n",
        "        self.e = e\n",
        "        self.env = gym.make(environment, render_mode=\"rgb_array\")\n",
        "\n",
        "        # Initialize the Q table\n",
        "        # If the environment has a terminal state, set its Q value to zero\n",
        "        # <-- CREATE AND INITIALIZE THE Q TABLE HERE -->\n",
        "\n",
        "    def learn(self, n_steps):\n",
        "        \"\"\"\n",
        "        This method is called to run Q-learning for n_steps time steps.\n",
        "        \"\"\"\n",
        "\n",
        "        # Create local copies of Q-learning parameters\n",
        "        g = self.g\n",
        "        a = self.a\n",
        "        e = self.e\n",
        "\n",
        "        # Start a new episode and loop for n_steps\n",
        "        done = True\n",
        "        for k in range(n_steps):\n",
        "\n",
        "            # Initiaize the new episode\n",
        "            if done is True:\n",
        "                obs = self.env.reset()[0]\n",
        "                # If this state is not yet in the Q-table, add it and\n",
        "                # initialize values to zero\n",
        "                # <-- ADD NEW Q VALUES HERE -->\n",
        "\n",
        "            # Select an action with epsilon-greedy action selection\n",
        "            # <-- SELECT AN ACTION HERE USING EPSILON-GREEDY -->\n",
        "\n",
        "            # Apply the selected action and observe the reward and next state\n",
        "            obs_prev = obs\n",
        "            obs, reward, done, info, other = self.env.step(action)\n",
        "            # Indicate whether the episode reached the terminal state\n",
        "            if done is True:\n",
        "                obs = \"done\"\n",
        "\n",
        "            # If the next state is not yet in the Q-table, add it and\n",
        "            # initialize values to zero\n",
        "            # <-- ADD NEW Q VALUES HERE -->\n",
        "\n",
        "            # Update the Q value for the previous state and selected action\n",
        "            # <-- UPDATE EXISTING VALUES HERE -->\n",
        "\n",
        "    def close(self):\n",
        "        # Close the environment\n",
        "        self.env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now let the Q-learning algorithm learn how to solve the Taxi environment by interacting with the environment for 500K time steps."
      ],
      "metadata": {
        "id": "J5g6uqs7MINb"
      },
      "id": "J5g6uqs7MINb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a19cf0-3fb4-4aa0-97e4-61c702594fa1",
      "metadata": {
        "id": "a1a19cf0-3fb4-4aa0-97e4-61c702594fa1"
      },
      "outputs": [],
      "source": [
        "learner = QLearner(\"Taxi-v3\")\n",
        "learner.learn(500000)\n",
        "learner.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1eacdc6-5588-4d29-816a-678d213ae9df",
      "metadata": {
        "id": "b1eacdc6-5588-4d29-816a-678d213ae9df"
      },
      "outputs": [],
      "source": [
        "action_desc = {\n",
        "    0: \"Move south (down)\",\n",
        "    1: \"Move north (up)\",\n",
        "    2: \"Move east (right)\",\n",
        "    3: \"Move west (left)\",\n",
        "    4: \"Pickup passenger\",\n",
        "    5: \"Drop off passenger\"\n",
        "}\n",
        "\n",
        "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
        "\n",
        "obs = env.reset()[0]\n",
        "plt.imshow(env.render())\n",
        "plt.show()\n",
        "\n",
        "for i in range(200):\n",
        "\n",
        "    # <-- SELECT AN ACTION HERE -->\n",
        "\n",
        "    obs, reward, done, info, other = env.step(action)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    plt.imshow(env.render())\n",
        "\n",
        "    rect = matplotlib.patches.Rectangle(\n",
        "        (150,0),\n",
        "        250,\n",
        "        75,\n",
        "        facecolor=\"#999999\",\n",
        "        edgecolor=\"#000000\"\n",
        "    )\n",
        "    ax = plt.gca()\n",
        "    ax.add_patch(rect)\n",
        "    plt.text(165,25,f\"State: {obs}\")\n",
        "    plt.text(165,45,f\"Action: {action_desc[action]}\")\n",
        "    plt.text(165,65,f\"Reward: {reward}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    if done:\n",
        "        obs = env.reset()[0]\n",
        "\n",
        "    time.sleep(0.5)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the learned policy now solves the taxi problem optimally in each run.\n",
        "\n",
        "It is important to note that Q-learning knows nothing a-priori about the overall objective of this task. It simply is selecting actions and observing the resulting immediate rewards and next states. By building up the Q table, we encode a policy that is capable of performung the task."
      ],
      "metadata": {
        "id": "FTd08AAVMW4-"
      },
      "id": "FTd08AAVMW4-"
    },
    {
      "cell_type": "markdown",
      "id": "40f78811-5d69-47ca-a6b9-93a0b7b6de05",
      "metadata": {
        "id": "40f78811-5d69-47ca-a6b9-93a0b7b6de05"
      },
      "source": [
        "---\n",
        "\n",
        "We can apply the Q-learning algoithm that we implemented to an entirely different task and still learn a poilicy for performing that task skillfully.\n",
        "\n",
        "Before finishing this demo, we will apply Q-learning to the Blackjack environment in Gymnasium. In this environment, a player plays a single hand of Blackjackagainst the dealer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0164e71-64f8-4f39-b297-efe2bd7de4a9",
      "metadata": {
        "id": "f0164e71-64f8-4f39-b297-efe2bd7de4a9"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Blackjack-v1\", render_mode=\"rgb_array\")\n",
        "obs = env.reset()[0]\n",
        "\n",
        "plt.imshow(env.render())\n",
        "plt.show()\n",
        "print(obs)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just as with the Taxi environment, we will let Q-learning learn how to play Blackjack by playing for 500K turns."
      ],
      "metadata": {
        "id": "Y1KXD591ODZU"
      },
      "id": "Y1KXD591ODZU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d2e22fe-2c45-4275-871b-9fe566f858ed",
      "metadata": {
        "id": "6d2e22fe-2c45-4275-871b-9fe566f858ed"
      },
      "outputs": [],
      "source": [
        "learner = QLearner(\"Blackjack-v1\")\n",
        "learner.learn(500000)\n",
        "learner.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the policy that we learned, we will play a fixed number of hands against the deealer and measure the fraction of hands that we win."
      ],
      "metadata": {
        "id": "TtO0cYVNOUhL"
      },
      "id": "TtO0cYVNOUhL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c397e32e-aa3a-4831-9b62-5eca74eb6abf",
      "metadata": {
        "id": "c397e32e-aa3a-4831-9b62-5eca74eb6abf"
      },
      "outputs": [],
      "source": [
        "def run_simulation(n_games, policy_func, **kwargs):\n",
        "    \"\"\"\n",
        "    Here we provide a reusable function for evaluating Blackjack policies.\n",
        "    For a given policy, this function will run n_games using the provided policy\n",
        "    function and return the fraction of games won.\n",
        "    \"\"\"\n",
        "\n",
        "    env = gym.make(\"Blackjack-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "    wins = 0\n",
        "    for i in range(n_games):\n",
        "\n",
        "        obs = env.reset()[0]\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy_func(obs, **kwargs)\n",
        "            obs, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        if reward > 0:\n",
        "            wins += 1\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    return wins/n_games\n",
        "\n",
        "#---\n",
        "\n",
        "def Q_policy(state, Q):\n",
        "    \"\"\"\n",
        "    This function implements a Blackjack policy from\n",
        "    a given Q function.\n",
        "    \"\"\"\n",
        "\n",
        "    if (state, 0) not in Q:\n",
        "        return env.action_space.sample()\n",
        "    elif Q[(state, 0)] > Q[(state, 1)]:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2dc36ce-e95a-48a8-bde4-91caee20383c",
      "metadata": {
        "id": "c2dc36ce-e95a-48a8-bde4-91caee20383c"
      },
      "outputs": [],
      "source": [
        "run_simulation(50000, Q_policy, Q=learner.Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a85056ac-9949-4e47-835e-422c5a01ee06",
      "metadata": {
        "id": "a85056ac-9949-4e47-835e-422c5a01ee06"
      },
      "source": [
        "Note that the known probability of winning a Blackjack hand if optimally played is 42.22% (we might see something higher here since there will be some estimation error from simulation). So, in this case it appears that Q-learning is able to learn an optimal (or very close to optimal) strategy for playing a single hand of Blackjack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b7ecb9-f4a3-4c92-bc74-efc7cfeff199",
      "metadata": {
        "id": "08b7ecb9-f4a3-4c92-bc74-efc7cfeff199"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}